# Classificador de Texto Ofensivo com LLM

##  Descrição do Projeto
Este projeto utiliza um **Large Language Model (LLM)** para **classificar automaticamente textos em duas categorias**:  
- **Ofensivo**  
- **Não ofensivo**

O objetivo é desenvolver uma ferramenta capaz de detectar linguagem inadequada, discurso de ódio ou conteúdo tóxico em textos de redes sociais, fóruns, chats e outras plataformas online.  

O modelo pode ser utilizado para **moderação automática**, **análise de sentimento com foco em segurança** e **monitoramento de reputação digital**.


##  Funcionalidades
- Classificação binária: **ofensivo / não ofensivo**  
- Suporte a diferentes idiomas (com foco inicial em **português**)  
- Baseado em modelos LLM modernos  
- Pipeline de pré-processamento de texto (limpeza, tokenização, normalização)  

